{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZce2Gs3nfgoXYvkNob6+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/burgerhaley97/Crime-Location-Prediction-Project/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Import/Preparation"
      ],
      "metadata": {
        "id": "6JTjMuoVEHjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import io"
      ],
      "metadata": {
        "id": "Bzzo1PadJxvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akGiqe25EGYG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/6655_HWs/Project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "-abFWO1YJvrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M-Y8vfLcd_3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Import csv\n",
        "df = pd.read_csv('CrimeData.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1kJOxr1-JQIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# keep only relevant columns\n",
        "df = df.drop(['Apartment Number', 'x', 'y', 'IDCol', 'Report Number', 'ObjectId',\n",
        "              'Day Occurred', 'Possible Date', 'Possible Time', 'NIBRS Code'], axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "HrU6vV4UJQLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Look at missing values per variable\n",
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count missing values per column\n",
        "missing_counts = df.isnull().sum()\n",
        "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "missing_counts.plot(kind='bar')\n",
        "plt.title('Missing Values per Column')\n",
        "plt.ylabel('Number of Missing Values')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FXAVWjK48--F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show rows where Neighborhood is missing\n",
        "df[df['Neighborhood'].isna()].head()\n"
      ],
      "metadata": {
        "id": "l9tGii3u9rRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Impute missing neighborhood values based on proximinty to other neighborhood\n",
        "centroids.\n",
        "'''\n",
        "# Drop rows with missing Neighborhoods to build the reference table\n",
        "known_locations = df.dropna(subset=['Neighborhood'])\n",
        "\n",
        "# Calculate average lat/lon for each neighborhood\n",
        "neighborhood_centroids = known_locations.groupby('Neighborhood')[['Latitude', 'Longitude']].mean()\n",
        "\n"
      ],
      "metadata": {
        "id": "VK2KHaZd-K4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np\n",
        "\n",
        "def impute_neighborhood(lat, lon, centroids):\n",
        "    distances = cdist([[lat, lon]], centroids[['Latitude', 'Longitude']].values)\n",
        "    min_idx = np.argmin(distances)\n",
        "    return centroids.index[min_idx]\n"
      ],
      "metadata": {
        "id": "rM1_HMGT-ZhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the rows with missing neighborhoods\n",
        "missing_rows = df[df['Neighborhood'].isna()]\n",
        "\n",
        "# Apply the imputation function\n",
        "df.loc[df['Neighborhood'].isna(), 'Neighborhood'] = missing_rows.apply(\n",
        "    lambda row: impute_neighborhood(row['Latitude'], row['Longitude'], neighborhood_centroids),\n",
        "    axis=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "yi5tuP4G-lLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Still missing Neighborhoods:\", df['Neighborhood'].isna().sum())"
      ],
      "metadata": {
        "id": "9IYUDart-fcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Impute missing values for NPU in a similar way since these are another type\n",
        "of neihgborhood unit.\n",
        "'''"
      ],
      "metadata": {
        "id": "8zKLKaUE_Brg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only rows where NPU is not missing\n",
        "known_npu = df.dropna(subset=['NPU'])\n",
        "\n",
        "# Compute average lat/lon for each NPU\n",
        "npu_centroids = known_npu.groupby('NPU')[['Latitude', 'Longitude']].mean()\n"
      ],
      "metadata": {
        "id": "DqfoAnD8_B0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "import numpy as np\n",
        "\n",
        "def impute_npu(lat, lon, centroids):\n",
        "    distances = cdist([[lat, lon]], centroids[['Latitude', 'Longitude']].values)\n",
        "    closest_idx = np.argmin(distances)\n",
        "    return centroids.index[closest_idx]\n"
      ],
      "metadata": {
        "id": "pEFC1O4j_K20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_npu_rows = df[df['NPU'].isna()]\n",
        "\n",
        "# Impute using nearest centroid\n",
        "df.loc[df['NPU'].isna(), 'NPU'] = missing_npu_rows.apply(\n",
        "    lambda row: impute_npu(row['Latitude'], row['Longitude'], npu_centroids),\n",
        "    axis=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "K29xRt0o_K81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Still missing NPUs:\", df['NPU'].isna().sum())"
      ],
      "metadata": {
        "id": "65ReRtxG_K_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Remove rows that have missing occur dates / times since there are very few.\n",
        "'''"
      ],
      "metadata": {
        "id": "oZu-mSqf_4AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['Occur Date', 'Occur Time'])\n"
      ],
      "metadata": {
        "id": "cWozzqhK_c7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Still missing Occur Date:\", df['Occur Date'].isna().sum())"
      ],
      "metadata": {
        "id": "NC9JrJtM_c4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Create Crime Frequency Feature:\n",
        "'''"
      ],
      "metadata": {
        "id": "krj8CaixAOHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the crime frequency per neighborhood feature\n",
        "df['Occur Date'] = pd.to_datetime(df['Occur Date'], format='mixed', errors='coerce')\n",
        "df = df.dropna(subset=['Neighborhood'])\n",
        "\n",
        "# Count number of crimes per neighborhood per day\n",
        "daily_counts = df.groupby(['Neighborhood', 'Occur Date']).size().reset_index(name='Daily_Crime_Count')\n",
        "\n",
        "# Now compute average daily crime per neighborhood\n",
        "avg_daily_crime = daily_counts.groupby('Neighborhood')['Daily_Crime_Count'].mean().reset_index(name='Avg_Crime_Per_Day')\n",
        "df = df.merge(avg_daily_crime, on='Neighborhood', how='left')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jv6EDCTKr6kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])\n"
      ],
      "metadata": {
        "id": "qI-K2hW-g8PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['Occur Date'])"
      ],
      "metadata": {
        "id": "T8pzeDQbiX6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many rows have missing Avg_Crime_Per_Day?\n",
        "print(\"Rows with missing Avg_Crime_Per_Day:\", df['Avg_Crime_Per_Day'].isna().sum())\n"
      ],
      "metadata": {
        "id": "LAxiJeFR8BMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check rows with NaT in 'Occur Time' or 'Occur Date'\n",
        "invalid_time_rows = df[pd.to_datetime(df['Occur Time'], format='%H:%M', errors='coerce').isna()]\n",
        "invalid_date_rows = df[pd.to_datetime(df['Occur Date'], errors='coerce').isna()]\n",
        "\n",
        "print(\"Problematic 'Occur Time' values:\")\n",
        "print(invalid_time_rows['Occur Time'].value_counts())\n",
        "\n",
        "print(\"Problematic 'Occur Date' values:\")\n",
        "print(invalid_date_rows['Occur Date'].value_counts())\n"
      ],
      "metadata": {
        "id": "ssunFOtQic97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows where Occur Time is invalid (i.e., can't be parsed)\n",
        "valid_times = pd.to_datetime(df['Occur Time'], format='%H:%M', errors='coerce').notna()\n",
        "\n",
        "# Keep only rows with valid Occur Time\n",
        "df = df[valid_times]\n",
        "\n",
        "# Now safely re-run the time conversion and feature extraction\n",
        "df['Occur Time'] = pd.to_datetime(df['Occur Time'], format='%H:%M')\n",
        "df['hour'] = df['Occur Time'].dt.hour\n",
        "df['dayofweek'] = df['Occur Date'].dt.dayofweek\n",
        "df['month'] = df['Occur Date'].dt.month\n"
      ],
      "metadata": {
        "id": "NhLmJosaiqfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])"
      ],
      "metadata": {
        "id": "u5M4OK5iiwAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ghDnCYzXHQ1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make New Crime Types"
      ],
      "metadata": {
        "id": "X9zr4bBQdObH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "Reclassify into our broader 3 crime types. Do SMOTE on this data.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "tKVdkGQgMzva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define your mapping\n",
        "crime_mapping = {\n",
        "    'LARCENY-FROM VEHICLE': 'Theft',\n",
        "    'LARCENY-NON VEHICLE': 'Theft',\n",
        "    'AUTO THEFT': 'Theft',\n",
        "    'BURGLARY': 'Burglary',\n",
        "    'AGG ASSAULT': 'Violence',\n",
        "    'ROBBERY': 'Violence',\n",
        "    'HOMICIDE': 'Violence'\n",
        "}\n",
        "\n",
        "# Create a new column with the mapped class\n",
        "df['Crime Category'] = df['Crime Type'].map(crime_mapping)\n"
      ],
      "metadata": {
        "id": "UVWjxuWYcLCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Crime Category'].value_counts())\n",
        "\n"
      ],
      "metadata": {
        "id": "xAVUSFyNcLLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, y='Crime Category', order=df['Crime Category'].value_counts().index, palette=\"Set2\")\n",
        "plt.title(\"Frequency of Crime Categories\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Crime Category\")\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N1VeB2zdy4yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 1. Define your features and target\n",
        "X = df.drop(columns=['Crime Type', 'Crime Category', 'Occur Time', 'Occur Date', 'Report Date'])\n",
        "y = df['Crime Category']\n",
        "\n",
        "# 2. Check current class distribution\n",
        "print(\"Original class distribution:\")\n",
        "print(y.value_counts())\n",
        "\n"
      ],
      "metadata": {
        "id": "xaooylH1cLQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "oQtKGuZwjNwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Apply SMOTE to upsample Burglary and Violence to 70% of Theft\n",
        "# Determine counts\n",
        "counts = y.value_counts()\n",
        "n_theft = counts['Theft']\n",
        "target_n = int(0.7 * n_theft)  # 70% of Theft\n",
        "\n",
        "# Create a dictionary for desired sampling strategy\n",
        "sampling_strategy = {\n",
        "    'Burglary': target_n,\n",
        "    'Violence': target_n\n",
        "}\n",
        "\n",
        "# 4. Apply SMOTE\n",
        "# Encode location columns\n",
        "X_encoded = pd.get_dummies(X, columns=['Neighborhood', 'NPU', 'Zone'], drop_first=True)\n",
        "\n",
        "# Drop the original string columns manually\n",
        "X_encoded = X_encoded.drop(columns=['Location'])\n"
      ],
      "metadata": {
        "id": "uPQ_uxuNe9j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_encoded.head()"
      ],
      "metadata": {
        "id": "Taz7i2t_jd9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=1891)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_encoded, y)\n",
        "\n",
        "# 5. Check new class distribution\n",
        "print(\"\\nAfter SMOTE class distribution:\")\n",
        "print(pd.Series(y_resampled).value_counts())"
      ],
      "metadata": {
        "id": "1ejeJYhugyPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_resampled_scaled = scaler.fit_transform(X_resampled)"
      ],
      "metadata": {
        "id": "T2lKxbZldctY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "wcXfIc4wQeTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Crime Type'].value_counts()"
      ],
      "metadata": {
        "id": "gCo-Sus3Qh7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df, y='Crime Type', order=df['Crime Type'].value_counts().index, palette=\"Set2\")\n",
        "plt.title(\"Frequency of Crime Types\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Crime Type\")\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9bGlWSXQM5So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Ensure date is parsed correctly\n",
        "df['Report Date'] = pd.to_datetime(df['Report Date'])\n",
        "\n",
        "# Step 2: Extract the year\n",
        "df['Year'] = df['Report Date'].dt.year\n",
        "\n",
        "# Step 3: Filter for just 2019 and 2020 (optional if there are other years)\n",
        "year_counts = df[df['Year'].isin([2019, 2020])]['Year'].value_counts().sort_index()\n",
        "\n",
        "# Step 4: Plot it\n",
        "year_counts.plot(kind='bar', color='cornflowerblue', edgecolor='black', figsize=(8, 6))\n",
        "plt.title(\"Total Crimes by Year (2019 & 2020)\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Number of Crimes\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Zfm8Lae8N_no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Ensure date column is in datetime format\n",
        "df['Report Date'] = pd.to_datetime(df['Report Date'])\n",
        "\n",
        "# Step 2: Extract day of the week (Monday = 0, Sunday = 6)\n",
        "df['Day of Week'] = df['Report Date'].dt.day_name()\n",
        "\n",
        "# Step 3: Create the frequency table\n",
        "day_counts = df['Day of Week'].value_counts().reindex([\n",
        "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
        "])\n",
        "\n",
        "# Step 4: Display the table\n",
        "print(day_counts.to_frame(name='Total Crimes'))\n",
        "\n",
        "day_counts.plot(kind='bar', color='mediumpurple', edgecolor='black', figsize=(8, 6))\n",
        "plt.title(\"Total Crimes by Day of the Week\")\n",
        "plt.ylabel(\"Number of Crimes\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LZW7uyHcPG2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a heat map type plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    data=df,\n",
        "    x=\"Longitude\",\n",
        "    y=\"Latitude\",\n",
        "    hue=\"crime_type_encoded\",\n",
        "    palette=\"Set2\",\n",
        "    s=100,\n",
        "    edgecolor=\"black\"\n",
        ")\n",
        "plt.title(\"Crime Incidents by Location and Type\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.legend(title=\"crime_type_encoded\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7eCv3atsDOsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium.plugins import MarkerCluster\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the base map centered around the average coordinates\n",
        "m = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()], zoom_start=12)\n",
        "\n",
        "# Define a color map for encoded crime types (0 to 6)\n",
        "crime_colors = {\n",
        "    0: 'red',       # e.g., BURGLARY\n",
        "    1: 'blue',      # e.g., LARCENY-FROM VEHICLE\n",
        "    2: 'orange',    # e.g., AUTO THEFT\n",
        "    3: 'purple',    # e.g., LARCENY-NON VEHICLE\n",
        "    4: 'green',     # assign colors as you like\n",
        "    5: 'brown',\n",
        "    6: 'pink'\n",
        "}\n",
        "\n",
        "\n",
        "# Add clustered markers colored by crime type\n",
        "marker_cluster = MarkerCluster().add_to(m)\n",
        "for _, row in df.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=(row['Latitude'], row['Longitude']),\n",
        "        radius=5,\n",
        "        color=crime_colors.get(row['crime_type_encoded'], 'gray'),\n",
        "        fill=True,\n",
        "        fill_opacity=0.7,\n",
        "        popup=row['crime_type_encoded']\n",
        "    ).add_to(marker_cluster)\n",
        "\n",
        "m\n"
      ],
      "metadata": {
        "id": "R-rVgAfUEKDr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from folium.plugins import HeatMap\n",
        "\n",
        "# Heatmap layer using lat/lon only\n",
        "heat_df = df[['Latitude', 'Longitude']]\n",
        "\n",
        "m = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()], zoom_start=12)\n",
        "HeatMap(heat_df.values, radius=15).add_to(m)\n",
        "m\n"
      ],
      "metadata": {
        "id": "UAtKrNq8GUgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "\n",
        "# Create the base map\n",
        "m = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()], zoom_start=12)\n",
        "\n",
        "# Add the heatmap\n",
        "heat_df = df[['Latitude', 'Longitude']]\n",
        "HeatMap(heat_df.values, radius=15).add_to(m)\n",
        "\n",
        "# Add custom HTML legend\n",
        "legend_html = \"\"\"\n",
        "<div style=\"\n",
        "    position: fixed;\n",
        "    bottom: 50px; left: 50px; width: 180px; height: 90px;\n",
        "    background-color: white;\n",
        "    border:2px solid grey;\n",
        "    z-index:9999;\n",
        "    font-size:14px;\n",
        "    padding: 10px;\">\n",
        "    <b>Crime Density</b><br>\n",
        "    <i style='background: red; width: 10px; height: 10px; display: inline-block;'></i> High<br>\n",
        "    <i style='background: orange; width: 10px; height: 10px; display: inline-block;'></i> Medium<br>\n",
        "    <i style='background: blue; width: 10px; height: 10px; display: inline-block;'></i> Low\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "m.get_root().html.add_child(folium.Element(legend_html))\n",
        "\n",
        "# Display in notebook (works in Jupyter and Colab)\n",
        "m\n"
      ],
      "metadata": {
        "id": "RF2TdHLM0Wfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eOf-8KladM5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try a Balanced Random Forest"
      ],
      "metadata": {
        "id": "A4RsCyPIEpHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Adjust for crazy class imbalance with random oversampling.\n",
        "'''"
      ],
      "metadata": {
        "id": "koGYv-cgEvO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize encoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit and transform your target column\n",
        "y_encoded = le.fit_transform(y_resampled)\n",
        "\n",
        "# Optional: see mapping of labels to encoded values\n",
        "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Label mapping:\", label_mapping)\n",
        "\n",
        "\n",
        "# Step 1: Split off 15% for the test set\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X_resampled_scaled, y_encoded, test_size=0.15, random_state=1891\n",
        ")\n",
        "\n",
        "# Step 2: Split the remaining 85% into train (70%) and validation (15%)\n",
        "# To get a 15% validation from the remaining 85%, use 0.1765 as the test_size:\n",
        "# (0.1765 * 0.85 ≈ 0.15 overall validation split)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1765, random_state=1891\n",
        ")"
      ],
      "metadata": {
        "id": "i9p0mQQVG8PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrain on the best parameters\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "\n",
        "# Create the final model with best-found parameters\n",
        "final_model = BalancedRandomForestClassifier(\n",
        "    sampling_strategy='auto',\n",
        "    n_estimators=200,\n",
        "    min_samples_split=2,\n",
        "    max_features='sqrt',\n",
        "    max_depth=10,\n",
        "    random_state=1891,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the final model\n",
        "final_model.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "EiRRtGC3EvdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = final_model.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "xcvlJJwTEvgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# LabelEncoder used earlier\n",
        "label_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
        "\n",
        "# Predict\n",
        "y_pred = final_model.predict(X_test)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Get label names in the same order as their encoded values\n",
        "labels = [label_mapping[i] for i in sorted(label_mapping)]\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report with target names\n",
        "print(classification_report(y_test, y_pred, target_names=labels))\n"
      ],
      "metadata": {
        "id": "EV-4xTxOrGN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NN Model Building/Training"
      ],
      "metadata": {
        "id": "R9IiW5W_QijD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize encoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit and transform your target column\n",
        "y_encoded = le.fit_transform(y_resampled)\n",
        "\n",
        "# Optional: see mapping of labels to encoded values\n",
        "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "print(\"Label mapping:\", label_mapping)\n",
        "\n",
        "\n",
        "# Step 1: Split off 15% for the test set\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X_resampled_scaled, y_encoded, test_size=0.15, random_state=1891\n",
        ")\n",
        "\n",
        "# Step 2: Split the remaining 85% into train (70%) and validation (15%)\n",
        "# To get a 15% validation from the remaining 85%, use 0.1765 as the test_size:\n",
        "# (0.1765 * 0.85 ≈ 0.15 overall validation split)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1765, random_state=1891\n",
        ")\n"
      ],
      "metadata": {
        "id": "kYIF2qM3wW-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZL0y1HgYOTnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the learning rate used by Adam\n",
        "print(\"Learning rate:\", model.optimizer.learning_rate.numpy())\n"
      ],
      "metadata": {
        "id": "AhdrlAP_PHor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Compute weights for each class\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "# Convert to dict format expected by model.fit()\n",
        "class_weights = {i : w for i, w in enumerate(class_weights_array)}\n",
        "\n",
        "print(\"Class weights:\", class_weights)\n"
      ],
      "metadata": {
        "id": "VM8c0A40tDMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and store history\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    class_weight=class_weights\n",
        ")"
      ],
      "metadata": {
        "id": "p95Ni_p2tH75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8eLVNVRcPtWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BpUo_z2WOeoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Try a deeper model with dropout and batch normalization (no class weights).\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "hgYJkg3YX6fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "L0TcSiSygX_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree('tuner_results/lr_batchsize_tuning')  # Deletes old tuner state\n",
        "\n",
        "# Then rerun tuner = RandomSearch(...) and tuner.search(...) as usual\n"
      ],
      "metadata": {
        "id": "MhUTDD1DtVp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_tuner import RandomSearch, HyperParameters\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_tuner import RandomSearch\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define model-building function\n",
        "def build_model(hp):\n",
        "    model = Sequential([\n",
        "        Input(shape=(X_train.shape[1],)),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(3, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Tune learning rate\n",
        "    lr = hp.Choice('learning_rate', values=[1e-4, 3e-4, 1e-3, 3e-3])\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Set up tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='tuner_results',\n",
        "    project_name='lr_batchsize_tuning'\n",
        ")\n",
        "\n",
        "# Define a separate HP space for batch_size tuning\n",
        "hp = HyperParameters()\n",
        "batch_size = hp.Choice('batch_size', values=[16, 32, 64, 128])\n",
        "\n",
        "# Search\n",
        "tuner.search(X_train, y_train,\n",
        "             epochs=8,\n",
        "             validation_split=0.1,\n",
        "             batch_size=batch_size,\n",
        "             class_weight=class_weights)\n",
        "\n",
        "# Get best model and hyperparameters\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hp = tuner.get_best_hyperparameters()[0]\n",
        "\n",
        "print(\"Best learning rate:\", best_hp.get('learning_rate'))\n"
      ],
      "metadata": {
        "id": "GRuX0bqDs4w4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild the model using the best hyperparameters\n",
        "final_model = build_model(best_hp)\n",
        "\n",
        "history = final_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    validation_split=0.1,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "fDzyFt7Uhlwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_QTEE9UAiAIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test data\n",
        "test_loss, test_accuracy = final_model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "saPXGD_ViCej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Predict class probabilities\n",
        "y_pred_probs = final_model.predict(X_test)\n",
        "\n",
        "# Convert probabilities to class predictions\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Get index-to-label mapping from label encoder\n",
        "label_map = {'Theft': 0, 'Burglary': 1, 'Violence': 2}\n",
        "index_to_label = {v: k for k, v in label_map.items()}\n",
        "labels = [index_to_label[i] for i in sorted(index_to_label)]\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, y_pred, target_names=labels))\n"
      ],
      "metadata": {
        "id": "GTN8_CFkirwI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}